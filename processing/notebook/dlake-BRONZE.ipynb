{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bca876f4-4490-4966-904e-946c5a7b1ddf",
   "metadata": {},
   "source": [
    "# <b style=\"color: white; background-color: #00bbff; padding: 5px 10px; border-radius: 5px;\">LIBRARY and SETTINGS</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d55361d5-03f2-4782-85be-ee6adbf04afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"level\": \"INFO\", \"message\": \"üîê Conectando ao endpoint: http://minio:9000\"}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from dotenv import load_dotenv\n",
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "from botocore.config import Config\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# -------------------------\n",
    "# Configura√ß√£o Avan√ßada de Logging\n",
    "# -------------------------\n",
    "def setup_logger():\n",
    "    log_directory = \"/opt/notebook/logs\"\n",
    "    os.makedirs(log_directory, exist_ok=True)\n",
    "    log_filename = f\"bronze_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "    log_file = os.path.join(log_directory, log_filename)\n",
    "    \n",
    "    logger = logging.getLogger(\"minio_silver\")\n",
    "    if logger.handlers:\n",
    "        logger.handlers = []\n",
    "    \n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter(fmt='{\"level\": \"%(levelname)s\", \"message\": \"%(message)s\"}')\n",
    "    \n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(formatter)\n",
    "    \n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    \n",
    "    logger.addHandler(console_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "\n",
    "logger = setup_logger()\n",
    "\n",
    "# Carrega vari√°veis de ambiente\n",
    "load_dotenv()\n",
    "s3_endpoint = os.getenv(\"S3_ENDPOINT\")\n",
    "s3_access_key = os.getenv(\"S3_ACCESS_KEY\")\n",
    "s3_secret_key = os.getenv(\"S3_SECRET_KEY\")\n",
    "\n",
    "logger.info(f\"üîê Conectando ao endpoint: {s3_endpoint}\")\n",
    "\n",
    "# -------------------------\n",
    "# Configura√ß√£o do Cliente S3\n",
    "# -------------------------\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=s3_endpoint,\n",
    "    aws_access_key_id=s3_access_key,\n",
    "    aws_secret_access_key=s3_secret_key,\n",
    "    config=Config(\n",
    "        signature_version=\"s3v4\",\n",
    "        retries={\n",
    "            'max_attempts': 5,\n",
    "            'mode': 'standard'\n",
    "        }\n",
    "    ),\n",
    "    region_name=\"us-east-1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79661e88-8613-4111-a9b9-0536de93b36c",
   "metadata": {},
   "source": [
    "# <b style=\"color: white; background-color: #00bbff; padding: 5px 10px; border-radius: 5px;\">FUNCTIONS</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68f915ed-cccc-45b1-af96-7fcb91555425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Fun√ß√µes Auxiliares\n",
    "# -------------------------\n",
    "def list_csv_files_recursive(bucket, prefix=\"\"):\n",
    "    \"\"\"Lista arquivos CSV de forma recursiva com pagina√ß√£o otimizada\"\"\"\n",
    "    csv_files = []\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    operation_parameters = {\n",
    "        'Bucket': bucket,\n",
    "        'Prefix': prefix,\n",
    "        'Delimiter': '/'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        for page in paginator.paginate(**operation_parameters):\n",
    "            csv_files.extend([\n",
    "                obj['Key'] for obj in page.get('Contents', [])\n",
    "                if obj['Key'].endswith('.csv')\n",
    "            ])\n",
    "            \n",
    "            for subfolder in page.get('CommonPrefixes', []):\n",
    "                csv_files.extend(list_csv_files_recursive(bucket, subfolder['Prefix']))\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erro ao listar arquivos: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    return csv_files\n",
    "\n",
    "def optimize_iceberg_table(spark, table_path):\n",
    "    \"\"\"Executa rotinas de otimiza√ß√£o para tabela Iceberg com sintaxe compat√≠vel\"\"\"\n",
    "    try:\n",
    "        # 1. Compacta√ß√£o de arquivos de dados (sem coment√°rios no SQL)\n",
    "        spark.sql(f\"\"\"\n",
    "            CALL local.system.rewrite_data_files(\n",
    "                table => '{table_path}',\n",
    "                options => map(\n",
    "                    'target-file-size-bytes', '67108864',\n",
    "                    'min-input-files', '5',\n",
    "                    'min-file-size-bytes', '33554432'\n",
    "                )\n",
    "            )\n",
    "        \"\"\")\n",
    "        logger.info(f\"‚úÖ Compacta√ß√£o conclu√≠da para {table_path}\")\n",
    "\n",
    "        # 2. Expurgo de snapshots antigos\n",
    "        retention_days = 7\n",
    "        cutoff_date = (datetime.now() - timedelta(days=retention_days)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        spark.sql(f\"\"\"\n",
    "            CALL local.system.expire_snapshots(\n",
    "                table => '{table_path}',\n",
    "                older_than => timestamp '{cutoff_date}',\n",
    "                retain_last => 3\n",
    "            )\n",
    "        \"\"\")\n",
    "        logger.info(f\"üóëÔ∏è Snapshots antigos removidos para {table_path}\")\n",
    "\n",
    "        # 3. Limpeza de arquivos √≥rf√£os\n",
    "        spark.sql(f\"\"\"\n",
    "            CALL local.system.remove_orphan_files(\n",
    "                table => '{table_path}',\n",
    "                older_than => timestamp '{cutoff_date}'\n",
    "            )\n",
    "        \"\"\")\n",
    "        logger.info(f\"üßπ Arquivos √≥rf√£os removidos para {table_path}\")\n",
    "\n",
    "        # 4. Otimiza√ß√£o de metadados\n",
    "        spark.sql(f\"CALL local.system.rewrite_manifests('{table_path}')\")\n",
    "        logger.info(f\"üì¶ Manifestos otimizados para {table_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Falha na otimiza√ß√£o de {table_path}: {str(e)}\")\n",
    "        raise   \n",
    "# -------------------------\n",
    "# Configura√ß√£o do Spark com Otimiza√ß√µes\n",
    "# -------------------------\n",
    "def create_spark_session():\n",
    "    \"\"\"Cria uma SparkSession otimizada para Iceberg com MinIO\"\"\"\n",
    "    from pyspark import SparkConf\n",
    "    \n",
    "    # Configura√ß√£o inicial para controle de logs\n",
    "    conf = SparkConf()\n",
    "    conf.set(\"spark.logConf\", \"false\")\n",
    "    conf.set(\"spark.ui.showConsoleProgress\", \"false\")\n",
    "    conf.set(\"spark.driver.extraJavaOptions\", \"-Dlog4j.configuration=file:/opt/spark/conf/log4j.properties\")\n",
    "    \n",
    "    # Verifica se os JARs existem\n",
    "    iceberg_jar = \"/opt/spark/jars/iceberg-spark-runtime-3.5_2.12-1.6.0.jar\"\n",
    "    required_jars = [\n",
    "        iceberg_jar,\n",
    "        \"/opt/spark/jars/hadoop-aws-3.3.4.jar\"\n",
    "    ]\n",
    "    \n",
    "    # Configura a SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .config(conf=conf) \\\n",
    "        .appName(\"IcebergOptimizedPipeline\") \\\n",
    "        .config(\"spark.jars\", \",\".join([j for j in required_jars if os.path.exists(j)])) \\\n",
    "        .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "        .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "        .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
    "        .config(\"spark.sql.catalog.local.warehouse\", \"s3a://datalake/iceberg\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", s3_endpoint) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", s3_access_key) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", s3_secret_key) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.fast.upload\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.fast.upload.buffer\", \"disk\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.maximum\", \"100\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.threads.max\", \"20\") \\\n",
    "        .config(\"spark.sql.catalog.local.default-namespace\", \"default\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "        .config(\"spark.default.parallelism\", \"4\") \\\n",
    "        .config(\"spark.sql.iceberg.handle-timestamp-without-timezone\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Configura√ß√£o adicional de logs\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    return spark\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c026525-34e4-4e29-8a7f-2de54ce8c0c3",
   "metadata": {},
   "source": [
    "# <b style=\"color: white; background-color: #00bbff; padding: 5px 10px; border-radius: 5px;\">EXECUTION</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6275b73d-eba0-42d6-9b81-030c8bd4c295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR StatusLogger Reconfiguration failed: No configuration found for '25af5db5' at 'null' in 'null'\n",
      "ERROR StatusLogger Reconfiguration failed: No configuration found for 'Default' at 'null' in 'null'\n",
      "25/09/13 19:01:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "{\"level\": \"INFO\", \"message\": \"üìÇ Total de arquivos encontrados: 2\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üîß Processando tabela: clientes (1 arquivos)\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üíæ Dados atualizados na tabela local.bronze.clientes\"}\n",
      "{\"level\": \"INFO\", \"message\": \"‚úÖ Compacta√ß√£o conclu√≠da para local.bronze.clientes\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üóëÔ∏è Snapshots antigos removidos para local.bronze.clientes\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üßπ Arquivos √≥rf√£os removidos para local.bronze.clientes\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üì¶ Manifestos otimizados para local.bronze.clientes\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üîß Processando tabela: vendas (1 arquivos)\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üíæ Dados atualizados na tabela local.bronze.vendas\"}\n",
      "{\"level\": \"INFO\", \"message\": \"‚úÖ Compacta√ß√£o conclu√≠da para local.bronze.vendas\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üóëÔ∏è Snapshots antigos removidos para local.bronze.vendas\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üßπ Arquivos √≥rf√£os removidos para local.bronze.vendas\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üì¶ Manifestos otimizados para local.bronze.vendas\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üèÅ Processamento conclu√≠do\"}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------\n",
    "# Processamento Principal\n",
    "# -------------------------\n",
    "def main():\n",
    "    spark = create_spark_session()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    \n",
    "    bucket_name = \"ingestion\"\n",
    "    try:\n",
    "        csv_files = list_csv_files_recursive(bucket_name)\n",
    "        if not csv_files:\n",
    "            logger.info(\"Nenhum arquivo .csv encontrado. Encerrando.\")\n",
    "            return\n",
    "        \n",
    "        logger.info(f\"üìÇ Total de arquivos encontrados: {len(csv_files)}\")\n",
    "        \n",
    "        # Agrupamento por prefixo (nome da tabela)\n",
    "        prefix_groups = defaultdict(list)\n",
    "        for file in csv_files:\n",
    "            folder_name = file.split('/')[-2] if '/' in file else \"root\"\n",
    "            prefix_groups[folder_name].append(file)\n",
    "        \n",
    "        # Processamento para cada tabela\n",
    "        for prefix, files in prefix_groups.items():\n",
    "            table_path = f\"local.bronze.{prefix}\"\n",
    "            file_paths = [f\"s3a://{bucket_name}/{file}\" for file in files]\n",
    "            \n",
    "            try:\n",
    "                logger.info(f\"üîß Processando tabela: {prefix} ({len(files)} arquivos)\")\n",
    "                \n",
    "                # Leitura dos arquivos\n",
    "                df = spark.read.option(\"header\", \"true\").csv(file_paths)\n",
    "                \n",
    "                if df.count() == 0:\n",
    "                    logger.warning(f\"‚ö†Ô∏è Dados vazios para {prefix}. Pulando...\")\n",
    "                    continue\n",
    "                \n",
    "                # Adiciona metadados temporais\n",
    "                df = df.withColumn(\"created_at\", current_timestamp())\n",
    "                df = df.dropDuplicates([\"id\"]).filter(\"id IS NOT NULL\")\n",
    "                \n",
    "                # Cria√ß√£o da tabela com propriedades otimizadas\n",
    "                cols = \", \".join(\n",
    "                    [f\"{field.name} STRING\" for field in df.schema.fields if field.name != \"created_at\"] +\n",
    "                    [\"created_at TIMESTAMP\"]\n",
    "                )\n",
    "                \n",
    "                spark.sql(f\"\"\"\n",
    "                    CREATE TABLE IF NOT EXISTS {table_path} (\n",
    "                        {cols}\n",
    "                    )\n",
    "                    USING iceberg\n",
    "                    PARTITIONED BY (days(created_at))\n",
    "                    TBLPROPERTIES (\n",
    "                        'write.format.default'='parquet',\n",
    "                        'write.parquet.compression-codec'='zstd',\n",
    "                        'write.target-file-size-bytes'='67108864',\n",
    "                        'commit.retry.num-retries'='5',\n",
    "                        'history.expire.max-snapshot-age-ms'='604800000',\n",
    "                        'write.metadata.delete-after-commit.enabled'='true',\n",
    "                        'write.metadata.previous-versions-max'='3'\n",
    "                    )\n",
    "                \"\"\")\n",
    "                \n",
    "                # Escreve dados com estrat√©gia MERGE\n",
    "                df.createOrReplaceTempView(\"temp_df\")\n",
    "                \n",
    "                spark.sql(f\"\"\"\n",
    "                    MERGE INTO {table_path} target\n",
    "                    USING temp_df source\n",
    "                    ON target.id = source.id\n",
    "                    WHEN MATCHED THEN UPDATE SET *\n",
    "                    WHEN NOT MATCHED THEN INSERT *\n",
    "                \"\"\")\n",
    "                \n",
    "                logger.info(f\"üíæ Dados atualizados na tabela {table_path}\")\n",
    "                \n",
    "                # Otimiza√ß√£o p√≥s-escrita\n",
    "                optimize_iceberg_table(spark, table_path)\n",
    "                \n",
    "                # Limpeza dos arquivos processados\n",
    "                for file in files:\n",
    "                    try:\n",
    "                        s3.delete_object(Bucket=bucket_name, Key=file)\n",
    "                        logger.debug(f\"Arquivo removido: {file}\")\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Erro ao remover {file}: {str(e)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå Falha no processamento de {prefix}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "    finally:\n",
    "        spark.stop()\n",
    "        logger.info(\"üèÅ Processamento conclu√≠do\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cac07f-bbf1-49ad-a018-14c2ca181b04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
