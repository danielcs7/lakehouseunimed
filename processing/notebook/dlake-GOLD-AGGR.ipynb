{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0b829ae-469e-46ca-a551-c2e88a79e689",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# <b style=\"color: white; background-color: #00bbff; padding: 5px 10px; border-radius: 5px;\">LIBRARY and SETTINGS</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84312832-8d49-4940-812c-1801c4b3f351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, count, avg, lit\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "# Carrega vari√°veis de ambiente\n",
    "load_dotenv()\n",
    "s3_endpoint = os.getenv(\"S3_ENDPOINT\")\n",
    "s3_access_key = os.getenv(\"S3_ACCESS_KEY\")\n",
    "s3_secret_key = os.getenv(\"S3_SECRET_KEY\")\n",
    "# -------------------------\n",
    "# Configura√ß√£o do Logging\n",
    "# -------------------------\n",
    "def setup_logger():\n",
    "    # Cria o nome do arquivo de log com timestamp\n",
    "    log_directory = \"/opt/notebook/logs/\"\n",
    "    os.makedirs(log_directory, exist_ok=True)  # Garante que o diret√≥rio existe\n",
    "    log_filename = f\"goldAgr_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "    log_file = os.path.join(log_directory, log_filename)\n",
    "    \n",
    "    # Evita m√∫ltiplos handlers\n",
    "    logger = logging.getLogger(\"minio_silver\")\n",
    "    if logger.handlers:  # Remove handlers existentes\n",
    "        logger.handlers = []\n",
    "    \n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # Formato do log\n",
    "    formatter = logging.Formatter(fmt='{\"level\": \"%(levelname)s\", \"message\": \"%(message)s\"}')\n",
    "    \n",
    "    # Handler para console\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(formatter)\n",
    "    \n",
    "    # Handler para arquivo\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    \n",
    "    # Adiciona ambos handlers\n",
    "    logger.addHandler(console_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "logger = setup_logger()\n",
    "\n",
    "def optimize_iceberg_table(spark, table_path):\n",
    "    \"\"\"Executa rotinas de otimiza√ß√£o para tabela Iceberg com sintaxe compat√≠vel\"\"\"\n",
    "    try:\n",
    "        # 1. Compacta√ß√£o de arquivos de dados (sem coment√°rios no SQL)\n",
    "        spark.sql(f\"\"\"\n",
    "            CALL local.system.rewrite_data_files(\n",
    "                table => '{table_path}',\n",
    "                options => map(\n",
    "                    'target-file-size-bytes', '67108864',\n",
    "                    'min-input-files', '5',\n",
    "                    'min-file-size-bytes', '33554432'\n",
    "                )\n",
    "            )\n",
    "        \"\"\")\n",
    "        logger.info(f\"‚úÖ Compacta√ß√£o conclu√≠da para {table_path}\")\n",
    "\n",
    "        # 2. Expurgo de snapshots antigos\n",
    "        retention_days = 7\n",
    "        cutoff_date = (datetime.now() - timedelta(days=retention_days)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        spark.sql(f\"\"\"\n",
    "            CALL local.system.expire_snapshots(\n",
    "                table => '{table_path}',\n",
    "                older_than => timestamp '{cutoff_date}',\n",
    "                retain_last => 3\n",
    "            )\n",
    "        \"\"\")\n",
    "        logger.info(f\"üóëÔ∏è Snapshots antigos removidos para {table_path}\")\n",
    "\n",
    "        # 3. Limpeza de arquivos √≥rf√£os\n",
    "        spark.sql(f\"\"\"\n",
    "            CALL local.system.remove_orphan_files(\n",
    "                table => '{table_path}',\n",
    "                older_than => timestamp '{cutoff_date}'\n",
    "            )\n",
    "        \"\"\")\n",
    "        logger.info(f\"üßπ Arquivos √≥rf√£os removidos para {table_path}\")\n",
    "\n",
    "        # 4. Otimiza√ß√£o de metadados\n",
    "        spark.sql(f\"CALL local.system.rewrite_manifests('{table_path}')\")\n",
    "        logger.info(f\"üì¶ Manifestos otimizados para {table_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Falha na otimiza√ß√£o de {table_path}: {str(e)}\")\n",
    "        raise \n",
    "# -------------------------\n",
    "# Configura√ß√£o do Spark com Otimiza√ß√µes\n",
    "# -------------------------\n",
    "def create_spark_session():\n",
    "    \"\"\"Cria uma SparkSession otimizada para Iceberg com MinIO\"\"\"\n",
    "    from pyspark import SparkConf\n",
    "    \n",
    "    # Configura√ß√£o inicial para controle de logs\n",
    "    conf = SparkConf()\n",
    "    conf.set(\"spark.logConf\", \"false\")\n",
    "    conf.set(\"spark.ui.showConsoleProgress\", \"false\")\n",
    "    conf.set(\"spark.driver.extraJavaOptions\", \"-Dlog4j.configuration=file:/opt/spark/conf/log4j.properties\")\n",
    "    \n",
    "    # Verifica se os JARs existem\n",
    "    iceberg_jar = \"/opt/spark/jars/iceberg-spark-runtime-3.5_2.12-1.6.0.jar\"\n",
    "    required_jars = [\n",
    "        iceberg_jar,\n",
    "        \"/opt/spark/jars/hadoop-aws-3.3.4.jar\"\n",
    "    ]\n",
    "    \n",
    "    # Configura a SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .config(conf=conf) \\\n",
    "        .appName(\"IcebergOptimizedPipeline\") \\\n",
    "        .config(\"spark.jars\", \",\".join([j for j in required_jars if os.path.exists(j)])) \\\n",
    "        .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "        .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "        .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
    "        .config(\"spark.sql.catalog.local.warehouse\", \"s3a://datalake/iceberg\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", s3_endpoint) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", s3_access_key) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", s3_secret_key) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.fast.upload\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.fast.upload.buffer\", \"disk\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.maximum\", \"100\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.threads.max\", \"20\") \\\n",
    "        .config(\"spark.sql.catalog.local.default-namespace\", \"default\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "        .config(\"spark.default.parallelism\", \"4\") \\\n",
    "        .config(\"spark.sql.iceberg.handle-timestamp-without-timezone\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Configura√ß√£o adicional de logs\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    return spark  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9b4058-f8cb-46f4-bc6f-41292b1088c4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# <b style=\"color: white; background-color: #00bbff; padding: 5px 10px; border-radius: 5px;\">FUNCTIONS AND EXECUTION</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "554629a4-f3a9-4bbc-83c3-3c4e94fbfb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"level\": \"INFO\", \"message\": \"üîß Processando agregados para an√°lise\"}\n",
      "{\"level\": \"INFO\", \"message\": \"‚úÖ Agregados processados com sucesso!\"}\n",
      "{\"level\": \"INFO\", \"message\": \"‚úÖ Compacta√ß√£o conclu√≠da para local.gold.agregado_vendas_categoria\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üóëÔ∏è Snapshots antigos removidos para local.gold.agregado_vendas_categoria\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üßπ Arquivos √≥rf√£os removidos para local.gold.agregado_vendas_categoria\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üì¶ Manifestos otimizados para local.gold.agregado_vendas_categoria\"}\n",
      "{\"level\": \"INFO\", \"message\": \"‚úÖ Compacta√ß√£o conclu√≠da para local.gold.agregado_vendas_cliente\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üóëÔ∏è Snapshots antigos removidos para local.gold.agregado_vendas_cliente\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üßπ Arquivos √≥rf√£os removidos para local.gold.agregado_vendas_cliente\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üì¶ Manifestos otimizados para local.gold.agregado_vendas_cliente\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üöÄ Processamento da camada gold conclu√≠do com sucesso!\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finally\n"
     ]
    }
   ],
   "source": [
    "# Processamento de agregados para an√°lise\n",
    "# -------------------------\n",
    "def process_agregados():\n",
    "    logger.info(\"üîß Processando agregados para an√°lise\")\n",
    "    spark = create_spark_session()\n",
    "    try:\n",
    "        # Verifica integridade das tabelas de entrada\n",
    "        for table in [\"local.gold.dim_clientes\", \"local.gold.fato_vendas\"]:\n",
    "            files = spark.sql(f\"SELECT * FROM {table}.files\").collect()\n",
    "            for file in files:\n",
    "                #logger.info(f\"Verificando {table}: Arquivo {file['file_path']}, tamanho: {file['file_size_in_bytes']} bytes\")\n",
    "                if file['file_size_in_bytes'] == 0:\n",
    "                    raise ValueError(f\"Arquivo Parquet vazio detectado em {table}: {file['file_path']}\")\n",
    "\n",
    "        # Carrega as tabelas gold\n",
    "        dim_clientes = spark.table(\"local.gold.dim_clientes\")\n",
    "        fato_vendas = spark.table(\"local.gold.fato_vendas\")\n",
    "        \n",
    "        # 1. Agregado de vendas por cliente\n",
    "        vendas_por_cliente = fato_vendas.join(\n",
    "            dim_clientes, \n",
    "            \"cliente_id\", \n",
    "            \"inner\"\n",
    "        ).groupBy(\n",
    "            \"cliente_id\",\n",
    "            \"nome_cliente\",\n",
    "            \"ano_venda\",\n",
    "            \"mes_venda\"\n",
    "        ).agg(\n",
    "            sum(\"total\").alias(\"total_gasto\"),\n",
    "            count(\"id\").alias(\"quantidade_compras\"),\n",
    "            avg(\"total\").alias(\"valor_medio_compra\")\n",
    "        ).withColumn(\"created_at\", lit(datetime.now()))\n",
    "        \n",
    "        # Cria/Atualiza tabela de agregados por cliente\n",
    "        spark.sql(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS local.gold.agregado_vendas_cliente (\n",
    "                cliente_id string,\n",
    "                nome_cliente string,\n",
    "                ano_venda int,\n",
    "                mes_venda int,\n",
    "                total_gasto double,\n",
    "                quantidade_compras long,\n",
    "                valor_medio_compra double,\n",
    "                created_at timestamp\n",
    "            )\n",
    "            USING iceberg\n",
    "            PARTITIONED BY (ano_venda, mes_venda)\n",
    "            TBLPROPERTIES (\n",
    "                'write.format.default'='parquet',\n",
    "                'write.parquet.compression-codec'='snappy',\n",
    "                'write.target-file-size-bytes'='134217728',\n",
    "                'commit.retry.num-retries'='10'\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        # Escreve os dados (sobrescreve parti√ß√µes)\n",
    "        vendas_por_cliente.writeTo(\"local.gold.agregado_vendas_cliente\").overwritePartitions()\n",
    "        \n",
    "        # Valida√ß√£o dos arquivos escritos\n",
    "        written_files = spark.sql(\"SELECT * FROM local.gold.agregado_vendas_cliente.files\").collect()\n",
    "        for file in written_files:\n",
    "            #logger.info(f\"Arquivo escrito em agregado_vendas_cliente: {file['file_path']}, tamanho: {file['file_size_in_bytes']} bytes\")\n",
    "            if file['file_size_in_bytes'] == 0:\n",
    "                raise ValueError(f\"Arquivo Parquet vazio detectado: {file['file_path']}\")\n",
    "        \n",
    "        # 2. Agregado de vendas por categoria\n",
    "        vendas_por_categoria = fato_vendas.groupBy(\n",
    "            \"categoria\",\n",
    "            \"ano_venda\",\n",
    "            \"mes_venda\"\n",
    "        ).agg(\n",
    "            sum(\"total\").alias(\"total_vendido\"),\n",
    "            count(\"id\").alias(\"quantidade_vendas\"),\n",
    "            avg(\"total\").alias(\"valor_medio_venda\")\n",
    "        ).withColumn(\"created_at\", lit(datetime.now()))\n",
    "        \n",
    "        # Cria/Atualiza tabela de agregados por categoria\n",
    "        spark.sql(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS local.gold.agregado_vendas_categoria (\n",
    "                categoria string,\n",
    "                ano_venda int,\n",
    "                mes_venda int,\n",
    "                total_vendido double,\n",
    "                quantidade_vendas long,\n",
    "                valor_medio_venda double,\n",
    "                created_at timestamp\n",
    "            )\n",
    "            USING iceberg\n",
    "            PARTITIONED BY (ano_venda, mes_venda)\n",
    "            TBLPROPERTIES (\n",
    "                'write.format.default'='parquet',\n",
    "                'write.parquet.compression-codec'='snappy',\n",
    "                'write.target-file-size-bytes'='134217728',\n",
    "                'commit.retry.num-retries'='10'\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        # Escreve os dados (sobrescreve parti√ß√µes)\n",
    "        vendas_por_categoria.writeTo(\"local.gold.agregado_vendas_categoria\").overwritePartitions()\n",
    "        \n",
    "        # Valida√ß√£o dos arquivos escritos\n",
    "        written_files = spark.sql(\"SELECT * FROM local.gold.agregado_vendas_categoria.files\").collect()\n",
    "        for file in written_files:\n",
    "            #logger.info(f\"Arquivo escrito em agregado_vendas_categoria: {file['file_path']}, tamanho: {file['file_size_in_bytes']} bytes\")\n",
    "            if file['file_size_in_bytes'] == 0:\n",
    "                raise ValueError(f\"Arquivo Parquet vazio detectado: {file['file_path']}\")\n",
    "        \n",
    "        logger.info(\"‚úÖ Agregados processados com sucesso!\")\n",
    "        table_path1 = 'local.gold.agregado_vendas_categoria'\n",
    "        table_path2 = 'local.gold.agregado_vendas_cliente'\n",
    "        # Otimiza√ß√£o p√≥s-escrita\n",
    "        optimize_iceberg_table(spark, table_path1)\n",
    "        optimize_iceberg_table(spark, table_path2)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Erro ao processar agregados: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# -------------------------\n",
    "# Execu√ß√£o principal\n",
    "# -------------------------\n",
    "def main():\n",
    "    try:\n",
    "        process_agregados()\n",
    "        logger.info(\"üöÄ Processamento da camada gold conclu√≠do com sucesso!\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Erro durante o processamento da camada gold: {str(e)}\")\n",
    "    finally:\n",
    "        print(\"finally\")\n",
    "        #spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4bf148-787b-4481-ae28-8c6206f3069a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
