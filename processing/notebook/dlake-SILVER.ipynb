{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67f8f962-cbf1-4351-ab24-b3e0331a4ad1",
   "metadata": {},
   "source": [
    "# <b style=\"color: white; background-color: #00bbff; padding: 5px 10px; border-radius: 5px;\">LIBRARY and SETTINGS</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fff61ed-d97c-4630-909c-1e37ab59b530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "# Carrega vari√°veis de ambiente\n",
    "load_dotenv()\n",
    "s3_endpoint = os.getenv(\"S3_ENDPOINT\")\n",
    "s3_access_key = os.getenv(\"S3_ACCESS_KEY\")\n",
    "s3_secret_key = os.getenv(\"S3_SECRET_KEY\")\n",
    "# -------------------------\n",
    "# Configura√ß√£o do Logging\n",
    "# -------------------------\n",
    "def setup_logger():\n",
    "    # Cria o nome do arquivo de log com timestamp\n",
    "    log_directory = \"/opt/notebook/logs/\"\n",
    "    os.makedirs(log_directory, exist_ok=True)\n",
    "    log_filename = f\"silver_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "    log_file = os.path.join(log_directory, log_filename)\n",
    "    \n",
    "    logger = logging.getLogger(\"minio_silver\")\n",
    "    if logger.handlers:\n",
    "        logger.handlers = []\n",
    "    \n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter(fmt='{\"level\": \"%(levelname)s\", \"message\": \"%(message)s\"}')\n",
    "    \n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(formatter)\n",
    "    \n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    \n",
    "    logger.addHandler(console_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "logger = setup_logger()\n",
    "\n",
    "# -------------------------\n",
    "# Nova Fun√ß√£o: Spark Session\n",
    "# -------------------------\n",
    "def create_spark_session():\n",
    "    \"\"\"Cria uma SparkSession otimizada para Iceberg com MinIO\"\"\"\n",
    "    from pyspark import SparkConf\n",
    "    \n",
    "    conf = SparkConf()\n",
    "    conf.set(\"spark.logConf\", \"false\")\n",
    "    conf.set(\"spark.ui.showConsoleProgress\", \"false\")\n",
    "    conf.set(\"spark.driver.extraJavaOptions\", \"-Dlog4j.configuration=file:/opt/spark/conf/log4j.properties\")\n",
    "    \n",
    "    iceberg_jar = \"/opt/spark/jars/iceberg-spark-runtime-3.5_2.12-1.6.0.jar\"\n",
    "    required_jars = [\n",
    "        iceberg_jar,\n",
    "        \"/opt/spark/jars/hadoop-aws-3.3.4.jar\"\n",
    "    ]\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "        .config(conf=conf) \\\n",
    "        .appName(\"IcebergOptimizedPipeline\") \\\n",
    "        .config(\"spark.jars\", \",\".join([j for j in required_jars if os.path.exists(j)])) \\\n",
    "        .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "        .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "        .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
    "        .config(\"spark.sql.catalog.local.warehouse\", \"s3a://datalake/iceberg\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", os.getenv(\"S3_ENDPOINT\")) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", os.getenv(\"S3_ACCESS_KEY\")) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", os.getenv(\"S3_SECRET_KEY\")) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.fast.upload\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.fast.upload.buffer\", \"disk\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.maximum\", \"100\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.threads.max\", \"20\") \\\n",
    "        .config(\"spark.sql.catalog.local.default-namespace\", \"default\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "        .config(\"spark.default.parallelism\", \"4\") \\\n",
    "        .config(\"spark.sql.iceberg.handle-timestamp-without-timezone\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb4a725-324b-4f6c-9e99-d19c8bd01c6e",
   "metadata": {},
   "source": [
    "# <b style=\"color: white; background-color: #00bbff; padding: 5px 10px; border-radius: 5px;\">FUNCTIONS AND EXECUTION</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e547ef59-a368-42ea-adc0-9fabf1913a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR StatusLogger Reconfiguration failed: No configuration found for '25af5db5' at 'null' in 'null'\n",
      "ERROR StatusLogger Reconfiguration failed: No configuration found for 'Default' at 'null' in 'null'\n",
      "25/09/13 19:10:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "{\"level\": \"INFO\", \"message\": \"üìã Tabelas encontradas na camada bronze: ['clientes', 'vendas']\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üîß Processando tabela para camada silver: clientes\"}\n",
      "{\"level\": \"INFO\", \"message\": \"‚úÖ Inseridos 0 novos registros.\"}\n",
      "{\"level\": \"INFO\", \"message\": \"‚úÖ Atualizados 14742 registros.\"}\n",
      "{\"level\": \"INFO\", \"message\": \"‚úÖ Compacta√ß√£o conclu√≠da para local.silver.clientes\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üóëÔ∏è Snapshots antigos removidos para local.silver.clientes\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üßπ Arquivos √≥rf√£os removidos para local.silver.clientes\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üì¶ Manifestos otimizados para local.silver.clientes\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üîß Processando tabela para camada silver: vendas\"}\n",
      "{\"level\": \"INFO\", \"message\": \"‚úÖ Inseridos 0 novos registros.\"}\n",
      "{\"level\": \"INFO\", \"message\": \"‚úÖ Atualizados 23038 registros.\"}\n",
      "{\"level\": \"INFO\", \"message\": \"‚úÖ Compacta√ß√£o conclu√≠da para local.silver.vendas\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üóëÔ∏è Snapshots antigos removidos para local.silver.vendas\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üßπ Arquivos √≥rf√£os removidos para local.silver.vendas\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üì¶ Manifestos otimizados para local.silver.vendas\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üöÄ Processamento da camada silver conclu√≠do.\"}\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Nova Fun√ß√£o: Otimiza√ß√£o\n",
    "# -------------------------\n",
    "def optimize_iceberg_table(spark, table_path):\n",
    "    \"\"\"Executa rotinas de otimiza√ß√£o para tabela Iceberg\"\"\"\n",
    "    try:\n",
    "        # 1. Compacta√ß√£o de arquivos\n",
    "        spark.sql(f\"\"\"\n",
    "            CALL local.system.rewrite_data_files(\n",
    "                table => '{table_path}',\n",
    "                options => map(\n",
    "                    'target-file-size-bytes', '67108864',\n",
    "                    'min-input-files', '5',\n",
    "                    'min-file-size-bytes', '33554432'\n",
    "                )\n",
    "            )\n",
    "        \"\"\")\n",
    "        logger.info(f\"‚úÖ Compacta√ß√£o conclu√≠da para {table_path}\")\n",
    "\n",
    "        # 2. Expurgo de snapshots\n",
    "        retention_days = 7\n",
    "        cutoff_date = (datetime.now() - timedelta(days=retention_days)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        spark.sql(f\"\"\"\n",
    "            CALL local.system.expire_snapshots(\n",
    "                table => '{table_path}',\n",
    "                older_than => timestamp '{cutoff_date}',\n",
    "                retain_last => 3\n",
    "            )\n",
    "        \"\"\")\n",
    "        logger.info(f\"üóëÔ∏è Snapshots antigos removidos para {table_path}\")\n",
    "\n",
    "        # 3. Limpeza de arquivos\n",
    "        spark.sql(f\"\"\"\n",
    "            CALL local.system.remove_orphan_files(\n",
    "                table => '{table_path}',\n",
    "                older_than => timestamp '{cutoff_date}'\n",
    "            )\n",
    "        \"\"\")\n",
    "        logger.info(f\"üßπ Arquivos √≥rf√£os removidos para {table_path}\")\n",
    "\n",
    "        # 4. Otimiza√ß√£o de metadados\n",
    "        spark.sql(f\"CALL local.system.rewrite_manifests('{table_path}')\")\n",
    "        logger.info(f\"üì¶ Manifestos otimizados para {table_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Falha na otimiza√ß√£o de {table_path}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# -------------------------\n",
    "# C√≥digo Principal\n",
    "# -------------------------\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    spark = create_spark_session()\n",
    "    \n",
    "    # Listagem das tabelas bronze\n",
    "    bronze_tables = spark.sql(\"SHOW TABLES IN local.bronze\").select(\"tableName\").rdd.flatMap(lambda x: x).collect()\n",
    "    logger.info(f\"üìã Tabelas encontradas na camada bronze: {bronze_tables}\")\n",
    "    \n",
    "    if not bronze_tables:\n",
    "        logger.warning(\"‚ö†Ô∏è Nenhuma tabela encontrada na camada bronze.\")\n",
    "        spark.stop()\n",
    "        exit(0)\n",
    "\n",
    "    # Processamento de cada tabela\n",
    "    for table in bronze_tables:\n",
    "        logger.info(f\"üîß Processando tabela para camada silver: {table}\")\n",
    "        \n",
    "        try:\n",
    "            # Carregamento dos dados\n",
    "            silver_source_df = spark.table(f\"local.bronze.{table}\") \\\n",
    "                .filter(f\"date(created_at) = '{datetime.now().strftime('%Y-%m-%d')}'\")\n",
    "            \n",
    "            # Valida√ß√£o dos dados\n",
    "            silver_source_df = silver_source_df.dropDuplicates([\"id\"]).filter(\"id IS NOT NULL\")\n",
    "            \n",
    "            if silver_source_df.count() == 0:\n",
    "                logger.info(f\"‚ÑπÔ∏è Nenhum dado recente em 'local.bronze.{table}'. Pulando.\")\n",
    "                continue\n",
    "\n",
    "            # Cria√ß√£o da tabela se necess√°rio\n",
    "            cols = \", \".join([f\"{field.name} {field.dataType.typeName()}\" for field in silver_source_df.schema.fields])\n",
    "            spark.sql(f\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS local.silver.{table} (\n",
    "                    {cols}\n",
    "                )\n",
    "                USING iceberg\n",
    "                PARTITIONED BY (days(created_at))\n",
    "                TBLPROPERTIES (\n",
    "                    'write.format.default'='parquet',\n",
    "                    'write.parquet.compression-codec'='snappy',\n",
    "                    'write.target-file-size-bytes'='134217728',\n",
    "                    'commit.retry.num-retries'='10'\n",
    "                )\n",
    "            \"\"\")\n",
    "\n",
    "            # Opera√ß√£o de Merge\n",
    "            silver_df = spark.table(f\"local.silver.{table}\")\n",
    "            silver_existing_ids = silver_df.select(\"id\").distinct()\n",
    "            \n",
    "            # Inser√ß√£o de novos registros\n",
    "            silver_insert_df = silver_source_df.join(silver_existing_ids, \"id\", \"left_anti\")\n",
    "            if silver_insert_df.count() > 0:\n",
    "                silver_insert_df.writeTo(f\"local.silver.{table}\").append()\n",
    "                logger.info(f\"‚úÖ Inseridos {silver_insert_df.count()} novos registros.\")\n",
    "            \n",
    "            # Atualiza√ß√£o de registros\n",
    "            silver_update_df = silver_source_df.join(silver_existing_ids, \"id\", \"inner\")\n",
    "            if silver_update_df.count() > 0:\n",
    "                silver_update_df.writeTo(f\"local.silver.{table}\").overwritePartitions()\n",
    "                logger.info(f\"‚úÖ Atualizados {silver_update_df.count()} registros.\")\n",
    "            \n",
    "            # Otimiza√ß√£o da tabela\n",
    "            optimize_iceberg_table(spark, f\"local.silver.{table}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Falha no processamento da tabela {table}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    logger.info(\"üöÄ Processamento da camada silver conclu√≠do.\")\n",
    "    spark.stop()\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"üî• Erro fatal: {str(e)}\")\n",
    "    if 'spark' in locals():\n",
    "        spark.stop()\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf7e3e4-31ff-46f1-bfb3-61455f67c529",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
