{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c946472f-766c-45b2-9692-47024654c96a",
   "metadata": {},
   "source": [
    "# <b style=\"color: white; background-color: #00bbff; padding: 5px 10px; border-radius: 5px;\">LIBRARY and SETTINGS</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd970305-8b7d-4750-bfda-2bac9c8db311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month, when\n",
    "import logging\n",
    "# Carrega vari√°veis de ambiente\n",
    "load_dotenv()\n",
    "s3_endpoint = os.getenv(\"S3_ENDPOINT\")\n",
    "s3_access_key = os.getenv(\"S3_ACCESS_KEY\")\n",
    "s3_secret_key = os.getenv(\"S3_SECRET_KEY\")\n",
    "\n",
    "# -------------------------\n",
    "# Configura√ß√£o do Logging\n",
    "# -------------------------\n",
    "def setup_logger():\n",
    "    # Cria o nome do arquivo de log com timestamp\n",
    "    log_directory = \"/opt/notebook/logs/\"\n",
    "    os.makedirs(log_directory, exist_ok=True)  # Garante que o diret√≥rio existe\n",
    "    log_filename = f\"gold_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "    log_file = os.path.join(log_directory, log_filename)\n",
    "    \n",
    "    # Evita m√∫ltiplos handlers\n",
    "    logger = logging.getLogger(\"minio_silver\")\n",
    "    if logger.handlers:  # Remove handlers existentes\n",
    "        logger.handlers = []\n",
    "    \n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # Formato do log\n",
    "    formatter = logging.Formatter(fmt='{\"level\": \"%(levelname)s\", \"message\": \"%(message)s\"}')\n",
    "    \n",
    "    # Handler para console\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(formatter)\n",
    "    \n",
    "    # Handler para arquivo\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    \n",
    "    # Adiciona ambos handlers\n",
    "    logger.addHandler(console_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "logger = setup_logger()\n",
    "\n",
    "def optimize_iceberg_table(spark, table_path):\n",
    "    \"\"\"Executa rotinas de otimiza√ß√£o para tabela Iceberg com sintaxe compat√≠vel\"\"\"\n",
    "    try:\n",
    "        # 1. Compacta√ß√£o de arquivos de dados (sem coment√°rios no SQL)\n",
    "        spark.sql(f\"\"\"\n",
    "            CALL local.system.rewrite_data_files(\n",
    "                table => '{table_path}',\n",
    "                options => map(\n",
    "                    'target-file-size-bytes', '67108864',\n",
    "                    'min-input-files', '5',\n",
    "                    'min-file-size-bytes', '33554432'\n",
    "                )\n",
    "            )\n",
    "        \"\"\")\n",
    "        logger.info(f\"‚úÖ Compacta√ß√£o conclu√≠da para {table_path}\")\n",
    "\n",
    "        # 2. Expurgo de snapshots antigos\n",
    "        retention_days = 7\n",
    "        cutoff_date = (datetime.now() - timedelta(days=retention_days)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        spark.sql(f\"\"\"\n",
    "            CALL local.system.expire_snapshots(\n",
    "                table => '{table_path}',\n",
    "                older_than => timestamp '{cutoff_date}',\n",
    "                retain_last => 3\n",
    "            )\n",
    "        \"\"\")\n",
    "        logger.info(f\"üóëÔ∏è Snapshots antigos removidos para {table_path}\")\n",
    "\n",
    "        # 3. Limpeza de arquivos √≥rf√£os\n",
    "        spark.sql(f\"\"\"\n",
    "            CALL local.system.remove_orphan_files(\n",
    "                table => '{table_path}',\n",
    "                older_than => timestamp '{cutoff_date}'\n",
    "            )\n",
    "        \"\"\")\n",
    "        logger.info(f\"üßπ Arquivos √≥rf√£os removidos para {table_path}\")\n",
    "\n",
    "        # 4. Otimiza√ß√£o de metadados\n",
    "        spark.sql(f\"CALL local.system.rewrite_manifests('{table_path}')\")\n",
    "        logger.info(f\"üì¶ Manifestos otimizados para {table_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Falha na otimiza√ß√£o de {table_path}: {str(e)}\")\n",
    "        raise   \n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Configura√ß√£o do Spark com Otimiza√ß√µes\n",
    "# -------------------------\n",
    "def create_spark_session():\n",
    "    \"\"\"Cria uma SparkSession otimizada para Iceberg com MinIO\"\"\"\n",
    "    from pyspark import SparkConf\n",
    "    \n",
    "    # Configura√ß√£o inicial para controle de logs\n",
    "    conf = SparkConf()\n",
    "    conf.set(\"spark.logConf\", \"false\")\n",
    "    conf.set(\"spark.ui.showConsoleProgress\", \"false\")\n",
    "    conf.set(\"spark.driver.extraJavaOptions\", \"-Dlog4j.configuration=file:/opt/spark/conf/log4j.properties\")\n",
    "    \n",
    "    # Verifica se os JARs existem\n",
    "    iceberg_jar = \"/opt/spark/jars/iceberg-spark-runtime-3.5_2.12-1.6.0.jar\"\n",
    "    required_jars = [\n",
    "        iceberg_jar,\n",
    "        \"/opt/spark/jars/hadoop-aws-3.3.4.jar\"\n",
    "    ]\n",
    "    \n",
    "    # Configura a SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .config(conf=conf) \\\n",
    "        .appName(\"IcebergOptimizedPipeline\") \\\n",
    "        .config(\"spark.jars\", \",\".join([j for j in required_jars if os.path.exists(j)])) \\\n",
    "        .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "        .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "        .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
    "        .config(\"spark.sql.catalog.local.warehouse\", \"s3a://datalake/iceberg\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", s3_endpoint) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", s3_access_key) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", s3_secret_key) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.fast.upload\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.fast.upload.buffer\", \"disk\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.maximum\", \"100\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.threads.max\", \"20\") \\\n",
    "        .config(\"spark.sql.catalog.local.default-namespace\", \"default\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "        .config(\"spark.default.parallelism\", \"4\") \\\n",
    "        .config(\"spark.sql.iceberg.handle-timestamp-without-timezone\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Configura√ß√£o adicional de logs\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    return spark        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2355d6b0-c14d-4747-aba6-cfd76da29833",
   "metadata": {},
   "source": [
    "# <b style=\"color: white; background-color: #00bbff; padding: 5px 10px; border-radius: 5px;\">FUNCTIONS AND EXECUTION</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7779971-efb7-49c1-9340-7e0db20dfc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR StatusLogger Reconfiguration failed: No configuration found for '25af5db5' at 'null' in 'null'\n",
      "ERROR StatusLogger Reconfiguration failed: No configuration found for 'Default' at 'null' in 'null'\n",
      "25/09/13 19:24:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "{\"level\": \"INFO\", \"message\": \"üîß Processando tabela de clientes para camada gold\"}\n",
      "{\"level\": \"INFO\", \"message\": \"‚úÖ Tabela gold de clientes processada com sucesso!\"}\n",
      "{\"level\": \"INFO\", \"message\": \"‚úÖ Compacta√ß√£o conclu√≠da para local.gold.dim_clientes\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üóëÔ∏è Snapshots antigos removidos para local.gold.dim_clientes\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üßπ Arquivos √≥rf√£os removidos para local.gold.dim_clientes\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üì¶ Manifestos otimizados para local.gold.dim_clientes\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üîß Processando tabela de vendas para camada gold\"}\n",
      "{\"level\": \"INFO\", \"message\": \"‚úÖ Tabela gold de vendas processada com sucesso!\"}\n",
      "{\"level\": \"INFO\", \"message\": \"‚úÖ Compacta√ß√£o conclu√≠da para local.gold.fato_vendas\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üóëÔ∏è Snapshots antigos removidos para local.gold.fato_vendas\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üßπ Arquivos √≥rf√£os removidos para local.gold.fato_vendas\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üì¶ Manifestos otimizados para local.gold.fato_vendas\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üèÅ Processamento base da camada gold conclu√≠do com sucesso!\"}\n",
      "{\"level\": \"INFO\", \"message\": \"üõë Sess√£o Spark finalizada\"}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------\n",
    "# Processamento de Clientes\n",
    "# -------------------------\n",
    "def process_clientes(spark):\n",
    "    logger.info(\"üîß Processando tabela de clientes para camada gold\")\n",
    "    \n",
    "    try:\n",
    "        clientes_df = spark.table(\"local.silver.clientes\")\n",
    "        \n",
    "        clientes_gold = clientes_df.select(\n",
    "            col(\"id\").alias(\"cliente_id\"),\n",
    "            col(\"nome\").alias(\"nome_cliente\"),\n",
    "            col(\"email\"),\n",
    "            col(\"data_cadastro\"),\n",
    "            when(col(\"status\") == \"ativo\", 1).otherwise(0).alias(\"ativo\"),\n",
    "            year(col(\"data_cadastro\")).alias(\"ano_cadastro\"),\n",
    "            month(col(\"data_cadastro\")).alias(\"mes_cadastro\"),\n",
    "            col(\"created_at\")\n",
    "        )\n",
    "\n",
    "        spark.sql(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS local.gold.dim_clientes (\n",
    "                cliente_id string,\n",
    "                nome_cliente string,\n",
    "                email string,\n",
    "                data_cadastro string,\n",
    "                ativo int,\n",
    "                ano_cadastro int,\n",
    "                mes_cadastro int,\n",
    "                created_at timestamp\n",
    "            )\n",
    "            USING iceberg\n",
    "            PARTITIONED BY (ano_cadastro, mes_cadastro)\n",
    "            TBLPROPERTIES (\n",
    "                'write.format.default' = 'parquet',\n",
    "                'write.parquet.compression-codec' = 'snappy',\n",
    "                'commit.retry.num-retries' = '10'\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "        clientes_gold.writeTo(\"local.gold.dim_clientes\").append()\n",
    "        logger.info(\"‚úÖ Tabela gold de clientes processada com sucesso!\")\n",
    "        \n",
    "        table_path = 'local.gold.dim_clientes'\n",
    "        # Otimiza√ß√£o p√≥s-escrita\n",
    "        optimize_iceberg_table(spark, table_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Erro ao processar clientes: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# -------------------------\n",
    "# Processamento de Vendas\n",
    "# -------------------------\n",
    "def process_vendas(spark):\n",
    "    logger.info(\"üîß Processando tabela de vendas para camada gold\")\n",
    "    \n",
    "    try:\n",
    "        vendas_df = spark.table(\"local.silver.vendas\")\n",
    "        \n",
    "        vendas_gold = vendas_df.select(\n",
    "            col(\"id\"),\n",
    "            col(\"cliente_id\"),\n",
    "            col(\"produto\"),\n",
    "            col(\"categoria\"),\n",
    "            col(\"quantidade\").cast(\"int\"),\n",
    "            col(\"preco_unitario\").cast(\"double\"),\n",
    "            col(\"total\").cast(\"double\"),\n",
    "            col(\"data_venda\"),\n",
    "            col(\"status\"),\n",
    "            year(col(\"data_venda\")).alias(\"ano_venda\"),\n",
    "            month(col(\"data_venda\")).alias(\"mes_venda\"),\n",
    "            col(\"created_at\")\n",
    "        )\n",
    "\n",
    "        spark.sql(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS local.gold.fato_vendas (\n",
    "                id string,\n",
    "                cliente_id string,\n",
    "                produto string,\n",
    "                categoria string,\n",
    "                quantidade int,\n",
    "                preco_unitario double,\n",
    "                total double,\n",
    "                data_venda string,\n",
    "                status string,\n",
    "                ano_venda int,\n",
    "                mes_venda int,\n",
    "                created_at timestamp\n",
    "            )\n",
    "            USING iceberg\n",
    "            PARTITIONED BY (ano_venda, mes_venda)\n",
    "            TBLPROPERTIES (\n",
    "                'write.format.default' = 'parquet',\n",
    "                'write.parquet.compression-codec' = 'snappy',\n",
    "                'commit.retry.num-retries' = '10'\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "        vendas_gold.writeTo(\"local.gold.fato_vendas\").append()\n",
    "        logger.info(\"‚úÖ Tabela gold de vendas processada com sucesso!\")\n",
    "        \n",
    "        table_path = 'local.gold.fato_vendas'\n",
    "        # Otimiza√ß√£o p√≥s-escrita\n",
    "        optimize_iceberg_table(spark, table_path)\n",
    "                \n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Erro ao processar vendas: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# -------------------------\n",
    "# Fun√ß√£o Principal\n",
    "# -------------------------\n",
    "def main():\n",
    "    spark = None\n",
    "    try:\n",
    "        spark = create_spark_session()\n",
    "        spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "        process_clientes(spark)\n",
    "        process_vendas(spark)\n",
    "        \n",
    "        logger.info(\"üèÅ Processamento base da camada gold conclu√≠do com sucesso!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Erro geral: {str(e)}\")\n",
    "    finally:\n",
    "        if spark:\n",
    "            spark.stop()\n",
    "            logger.info(\"üõë Sess√£o Spark finalizada\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b479e30c-63b4-4f6d-be64-32789cccfc75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
